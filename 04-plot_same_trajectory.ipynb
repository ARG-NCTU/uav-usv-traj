{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efec0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import movingpandas as mpd\n",
    "mpd.show_versions()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee738a",
   "metadata": {},
   "source": [
    "Mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffb05a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create mapping file: output_file.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_and_record_csv_files(folder_path, output_csv):\n",
    "    \n",
    "    files = os.listdir(folder_path)\n",
    "    # filter out .csv files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "    # create a new .csv file to store the mapping\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Original Filename\", \"New ID\"])\n",
    "        \n",
    "        for idx, csv_file in enumerate(csv_files, start=1):\n",
    "            writer.writerow([csv_file, idx])\n",
    "\n",
    "\n",
    "folder_path = './data/csv' \n",
    "output_csv = 'output_file.csv'\n",
    "\n",
    "filter_and_record_csv_files(folder_path, output_csv)\n",
    "print(f\"create mapping file: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d2f19",
   "metadata": {},
   "source": [
    "Calculate ATE and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec5cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv files done \n",
      "Id: 6006 and 6007\tATE (RMSE): 0.02 km\n",
      "Id: 4763 and 4764\tATE (RMSE): 0.02 km\n",
      "Id:  115 and  116\tATE (RMSE): 0.03 km\n",
      "Id: 3553 and 3554\tATE (RMSE): 0.04 km\n",
      "Id: 4711 and 4712\tATE (RMSE): 0.04 km\n",
      "Id: 3688 and 3689\tATE (RMSE): 0.05 km\n",
      "Id: 1390 and 1391\tATE (RMSE): 0.05 km\n",
      "Id: 4013 and 4014\tATE (RMSE): 0.05 km\n",
      "Id: 4093 and 4094\tATE (RMSE): 0.05 km\n",
      "Id: 3931 and 3932\tATE (RMSE): 0.06 km\n",
      "save to: sorted_rmse_results.csv\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  #radius of the Earth in km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def interpolate_trajectory(trajectory, num_points):\n",
    "    lat, lon = trajectory[:, 0], trajectory[:, 1]\n",
    "    t_original = np.linspace(0, 1, len(lat))\n",
    "    t_target = np.linspace(0, 1, num_points)\n",
    "    \n",
    "    interp_lat = interp1d(t_original, lat, kind='linear')\n",
    "    interp_lon = interp1d(t_original, lon, kind='linear')\n",
    "    \n",
    "    lat_target = interp_lat(t_target)\n",
    "    lon_target = interp_lon(t_target)\n",
    "    \n",
    "    return np.vstack((lat_target, lon_target)).T\n",
    "\n",
    "def calculate_ate(ground_truth, estimated):\n",
    "    num_points = max(len(ground_truth), len(estimated))\n",
    "    \n",
    "    ground_truth_interp = interpolate_trajectory(ground_truth, num_points)\n",
    "    estimated_interp = interpolate_trajectory(estimated, num_points)\n",
    "    \n",
    "    errors = np.array([haversine(lat1, lon1, lat2, lon2) for (lat1, lon1), (lat2, lon2) in zip(ground_truth_interp, estimated_interp)])\n",
    "    rmse = np.sqrt(np.mean(errors ** 2))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "#read mapping csv files\n",
    "files = 'output_file.csv'\n",
    "df = pd.read_csv(files)\n",
    "coordinates = []\n",
    "for index, row in df.iterrows():\n",
    "    original_filename = row.iloc[0]\n",
    "    file_id = row.iloc[1]\n",
    "\n",
    "    # read original csv file\n",
    "    file_path = './data/csv/' + original_filename\n",
    "    \n",
    "    df1 = pd.read_csv(file_path, delimiter=';')\n",
    "    # get the first trajectory coordinates\n",
    "    filtered_df = df1[df1['trajectory_id'] == 1][['X', 'Y']]\n",
    "\n",
    "    # to numpy array\n",
    "    coordinates.append(np.array(filtered_df))\n",
    "\n",
    "print('read csv files done ')\n",
    "rmse_records = []\n",
    "\n",
    "# calculate ATE for each pair of consecutive trajectories\n",
    "for i in range(1, len(coordinates)):\n",
    "    rmse = calculate_ate(coordinates[i-1], coordinates[i])\n",
    "    #print('Id:', i-1, 'and', i, '\\tATE (RMSE):', rmse, 'km')\n",
    "    rmse_records.append((i, i+1, rmse))\n",
    "# sort the results by RMSE\n",
    "rmse_records_sorted = sorted(rmse_records, key=lambda x: x[2])\n",
    "\n",
    "# output the top 10 results\n",
    "for idx, record in enumerate(rmse_records_sorted[:10]):\n",
    "    id1, id2, rmse = record\n",
    "    print(f'Id: {id1:4} and {id2:4}\\tATE (RMSE): {rmse:.2f} km')\n",
    "    if idx == 9:\n",
    "        break\n",
    "\n",
    "# save the sorted results to a csv file\n",
    "dfs = pd.DataFrame(rmse_records_sorted, columns=['Id1', 'Id2', 'ATE_RMSE_km'])\n",
    "csv_filename = 'sorted_rmse_results.csv'\n",
    "dfs.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f'save to: {csv_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b61996",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_file = './sorted_rmse_results.csv'\n",
    "filename_file = './output_file.csv'\n",
    "filename_map = {}\n",
    "pair = []\n",
    "# read mapping file and store the mapping in a dictionary\n",
    "with open(filename_file, mode='r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        new_id = int(row['New ID'])\n",
    "        original_filename = row['Original Filename']\n",
    "        filename_map[new_id] = original_filename\n",
    "count=0\n",
    "# read the sorted RMSE results and print the original filenames\n",
    "with open(rmse_file, mode='r', encoding='utf-8') as f:\n",
    "    \n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if count>31:\n",
    "            break\n",
    "        count+=1\n",
    "        id1 = int(row['Id1'])\n",
    "        id2 = int(row['Id2'])\n",
    "        filename_id1 = filename_map.get(id1, 'Unknown')\n",
    "        filename_id2 = filename_map.get(id2, 'Unknown')\n",
    "        \n",
    "        print(f'Id1: {id1}\\tOriginal Filename: {filename_id1}')\n",
    "        print(f'Id2: {id2}\\tOriginal Filename: {filename_id2}')\n",
    "        print()\n",
    "        pair.append((filename_id1, filename_id2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5140728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoviews as gv\n",
    "bing_maps_tile_source = gv.tile_sources.WMTS(\"http://ecn.t3.tiles.virtualearth.net/tiles/a{q}.jpeg?g=1\")\n",
    "hvplot_defaults = {'tiles':bing_maps_tile_source, 'frame_height':400, 'frame_width':400, 'colorbar':True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c425d9-ee8e-48f0-ac5b-de060cab7f9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df = pd.read_csv('data/csv/'+pair[i][0], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot1 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    df = pd.read_csv('data/csv/'+pair[i][1], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot2 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    print(pair[i][0], 'and', pair[i][1])\n",
    "    display(plot1 + plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94c450-a70e-41f9-9557-9efd847761b5",
   "metadata": {},
   "source": [
    "<mark>OVERLAPPING TRAJECTORIES sorted by ATE<mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38362780-fd60-459e-9ee4-018cb1804e33",
   "metadata": {},
   "source": [
    "![alt text](img/ATE01.png)\n",
    "![alt text](img/ATE02.png)\n",
    "![alt text](img/ATE03.png)\n",
    "![alt text](img/ATE04.png)\n",
    "![alt text](img/ATE05.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,10):\n",
    "    df = pd.read_csv('data/csv/'+pair[i][0], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot1 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    df = pd.read_csv('data/csv/'+pair[i][1], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot2 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    print(pair[i][0], 'and', pair[i][1])\n",
    "    display(plot1 + plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0757-a0cf-4d9b-b9c3-dbdf87aed726",
   "metadata": {},
   "source": [
    "![alt text](img/ATE06.png)\n",
    "![alt text](img/ATE07.png)\n",
    "![alt text](img/ATE08.png)\n",
    "![alt text](img/ATE09.png)\n",
    "![alt text](img/ATE10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b49508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,15):\n",
    "    df = pd.read_csv('data/csv/'+pair[i][0], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot1 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    df = pd.read_csv('data/csv/'+pair[i][1], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot2 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    print(pair[i][0], 'and', pair[i][1])\n",
    "    display(plot1 + plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e183f3-15df-4ec4-a479-4dc7451b92ac",
   "metadata": {},
   "source": [
    "![alt text](img/ATE11.png)\n",
    "![alt text](img/ATE12.png)\n",
    "![alt text](img/ATE13.png)\n",
    "![alt text](img/ATE14.png)\n",
    "![alt text](img/ATE15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2baf1-126f-4ee2-af97-e49f71a8abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15,20):\n",
    "    df = pd.read_csv('data/csv/'+pair[i][0], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot1 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    df = pd.read_csv('data/csv/'+pair[i][1], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot2 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    print(pair[i][0], 'and', pair[i][1])\n",
    "    display(plot1 + plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc1f4d-6d9d-4e5e-9350-e2ae127214e9",
   "metadata": {},
   "source": [
    "![alt text](img/ATE16.png)\n",
    "![alt text](img/ATE17.png)\n",
    "![alt text](img/ATE18.png)\n",
    "![alt text](img/ATE19.png)\n",
    "![alt text](img/ATE20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d90445-8a0b-49ec-b6bd-77e37235c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,25):\n",
    "    df = pd.read_csv('data/csv/'+pair[i][0], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot1 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    df = pd.read_csv('data/csv/'+pair[i][1], delimiter=';')\n",
    "    traj_collection = mpd.TrajectoryCollection(df, 'trajectory_id', t='t', x='X', y='Y')\n",
    "    plot2 = traj_collection.hvplot(title=str(traj_collection), line_width=[5,.8], **hvplot_defaults)\n",
    "    print(pair[i][0], 'and', pair[i][1])\n",
    "    display(plot1 + plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d631cf-1a9a-40f1-8f9d-8a7df5f350ae",
   "metadata": {},
   "source": [
    "![alt text](img/ATE21.png)\n",
    "![alt text](img/ATE22.png)\n",
    "![alt text](img/ATE23.png)\n",
    "![alt text](img/ATE24.png)\n",
    "![alt text](img/ATE25.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
